# Databricks Notebook: Common_Write_to_DB_Notebook
# Modified for Interactive Runtime Compatibility
# This notebook handles safe, sequential writing of alerts to the database.

import pandas as pd
from pyspark.sql.functions import col, lit, split, explode, regexp_replace
from pyspark.sql.types import StructType, StructField, StringType
import json
import datetime
import random

# Create widgets with default values for interactive testing
print("=== CREATING INTERACTIVE WIDGETS ===")

# Widget 1: JSON Alert Data
default_json = '''[{
    "SMBCAIDPrefix": "WIDGET_TEST_001",
    "ProductSymbol": "GBPJPY", 
    "SMBCALertTime": "2025-01-04 21:30:00.000",
    "SMBCLocalAlertTime": "2025-01-04 16:30:00.000",
    "SMBCAlertType": "Cross Product Manipulation",
    "IssuerName": "Widget Test Bank",
    "ShortAlertDescription": "Widget default test alert",
    "TotalScore": "95",
    "BusinessUnit": "Widget Trading Desk",
    "SMBCAlertGenerationDate": "2025-01-04 16:30:00.000",
    "ProductName": "GBP/JPY Cross Rate",
    "ProductID": "GBPJPY_WIDGET_001",
    "AccountID": "WIDGET_ACCT_001",
    "Exchange": "WIDGET_OTC",
    "AssetClass": "FX Cross",
    "LongDescription": "This is a comprehensive widget test alert with full data structure for interactive validation",
    "AnalyticsValue1": "Widget time difference: 38.7 seconds",
    "AnalyticsValue2": "Widget qualified trades: 15",
    "AnalyticsValue3": "Widget derivative size: 4200000.00",
    "AnalyticsValue4": "Widget avg trade size: 280000.00",
    "AnalyticsValue5": "Widget price volatility: 0.0031",
    "ThresholdValue1": "Widget derivative threshold: 1500000.00",
    "ThresholdValue2": "Widget lookback time: 240 seconds",
    "ThresholdValue3": "Widget underlying threshold: 750000.00",
    "ThresholdValue4": "Widget min trades: 7.00",
    "ScoreComponent1": "Widget time score: 32",
    "ScoreComponent2": "Widget volume score: 35", 
    "ScoreComponent3": "Widget pattern score: 28",
    "AlertRelatedListOrderClordIds": "WIDGET_ORD001,WIDGET_ORD002,WIDGET_ORD003",
    "AlertRelatedListExecutionClordIds": "WIDGET_EXEC001,WIDGET_EXEC002,WIDGET_EXEC003,WIDGET_EXEC004"
}]'''

dbutils.widgets.text("input_dataframe_json", default_json, "1. Alert JSON Data")

# Widget 2: Surveillance Run ID
dbutils.widgets.text("input_survid", "WIDGET_SURV_77777", "2. Surveillance Run ID")

# Widget 3: Dataset Run ID
dbutils.widgets.text("input_ds_runid", "WIDGET_DS_88888", "3. Dataset Run ID")

# Widget 4: Alert Type
dbutils.widgets.dropdown("input_alert_type", "Cross Product Manipulation", 
                        ["Cross Product Manipulation", "Customer Cross CM", "Test Alert Type"], 
                        "4. Alert Type")

print("âœ… WIDGETS CREATED - You can now modify values in the widget panel above")
print("âœ… DEFAULT VALUES SET - Ready for interactive testing")

# Get parameter values from widgets
try:
    input_dataframe_json_str = dbutils.widgets.get("input_dataframe_json")
    input_survid = dbutils.widgets.get("input_survid")
    input_ds_runid = dbutils.widgets.get("input_ds_runid")
    input_alert_type = dbutils.widgets.get("input_alert_type")
    
    print("=== PARAMETERS LOADED FROM WIDGETS ===")
    print(f"âœ… Successfully loaded all widget parameters")
    
except Exception as e:
    print("=== WIDGET PARAMETER ERROR ===")
    print(f"âŒ Error loading widget parameters: {e}")
    print("ðŸ”„ Using fallback default values")
    
    # Fallback default values
    input_dataframe_json_str = default_json
    input_survid = "FALLBACK_SURV_99999"
    input_ds_runid = "FALLBACK_DS_99999"
    input_alert_type = "Cross Product Manipulation"

print("=== COMMON WRITE TO DB NOTEBOOK STARTED ===")
print(f"Runtime Type: Interactive Runtime Enabled")
print(f"Received Surveillance ID: {input_survid}")
print(f"Received Dataset Run ID: {input_ds_runid}")
print(f"Received Alert Type: {input_alert_type}")
print(f"Received JSON Data Length: {len(input_dataframe_json_str)} characters")

# Reconstruct DataFrame from JSON string
try:
    # Check if input is empty or just empty brackets
    if not input_dataframe_json_str or input_dataframe_json_str.strip() in ['[]', '{}', 'null', '']:
        print("INPUT: Empty DataFrame received - no alerts to process")
        
        # Create empty schema matching AlertDetail table structure
        alert_schema = StructType([
            StructField("SMBCAIDPrefix", StringType(), True),
            StructField("ProductSymbol", StringType(), True),
            StructField("SMBCALertTime", StringType(), True),
            StructField("SMBCLocalAlertTime", StringType(), True),
            StructField("SMBCAlertType", StringType(), True),
            StructField("IssuerName", StringType(), True),
            StructField("ShortAlertDescription", StringType(), True),
            StructField("TotalScore", StringType(), True),
            StructField("BusinessUnit", StringType(), True),
            StructField("SMBCAlertGenerationDate", StringType(), True),
            StructField("ProductName", StringType(), True),
            StructField("ProductID", StringType(), True),
            StructField("AccountID", StringType(), True),
            StructField("Exchange", StringType(), True),
            StructField("AssetClass", StringType(), True),
            StructField("LongDescription", StringType(), True),
            StructField("AnalyticsValue1", StringType(), True),
            StructField("AnalyticsValue2", StringType(), True),
            StructField("AnalyticsValue3", StringType(), True),
            StructField("AnalyticsValue4", StringType(), True),
            StructField("AnalyticsValue5", StringType(), True),
            StructField("AnalyticsValue6", StringType(), True),
            StructField("AnalyticsValue7", StringType(), True),
            StructField("AnalyticsValue8", StringType(), True),
            StructField("AnalyticsValue9", StringType(), True),
            StructField("AnalyticsValue10", StringType(), True),
            StructField("AnalyticsValue11", StringType(), True),
            StructField("AnalyticsValue12", StringType(), True),
            StructField("AnalyticsValue13", StringType(), True),
            StructField("AnalyticsValue14", StringType(), True),
            StructField("AnalyticsValue15", StringType(), True),
            StructField("AnalyticsValue16", StringType(), True),
            StructField("AnalyticsValue17", StringType(), True),
            StructField("AnalyticsValue18", StringType(), True),
            StructField("AnalyticsValue19", StringType(), True),
            StructField("AnalyticsValue20", StringType(), True),
            StructField("ThresholdValue1", StringType(), True),
            StructField("ThresholdValue2", StringType(), True),
            StructField("ThresholdValue3", StringType(), True),
            StructField("ThresholdValue4", StringType(), True),
            StructField("ThresholdValue5", StringType(), True),
            StructField("ThresholdValue6", StringType(), True),
            StructField("ThresholdValue7", StringType(), True),
            StructField("ThresholdValue8", StringType(), True),
            StructField("ThresholdValue9", StringType(), True),
            StructField("ThresholdValue10", StringType(), True),
            StructField("ThresholdValue11", StringType(), True),
            StructField("ThresholdValue12", StringType(), True),
            StructField("ThresholdValue13", StringType(), True),
            StructField("ThresholdValue14", StringType(), True),
            StructField("ThresholdValue15", StringType(), True),
            StructField("ThresholdValue16", StringType(), True),
            StructField("ThresholdValue17", StringType(), True),
            StructField("ThresholdValue18", StringType(), True),
            StructField("ThresholdValue19", StringType(), True),
            StructField("ThresholdValue20", StringType(), True),
            StructField("ScoreComponent1", StringType(), True),
            StructField("ScoreComponent2", StringType(), True),
            StructField("ScoreComponent3", StringType(), True),
            StructField("ScoreComponent4", StringType(), True),
            StructField("ScoreComponent5", StringType(), True),
            StructField("ScoreComponent6", StringType(), True),
            StructField("ScoreComponent7", StringType(), True),
            StructField("ScoreComponent8", StringType(), True),
            StructField("ScoreComponent9", StringType(), True),
            StructField("ScoreComponent10", StringType(), True),
            StructField("ScoreComponent11", StringType(), True),
            StructField("ScoreComponent12", StringType(), True),
            StructField("ScoreComponent13", StringType(), True),
            StructField("ScoreComponent14", StringType(), True),
            StructField("ScoreComponent15", StringType(), True),
            StructField("ScoreComponent16", StringType(), True),
            StructField("ScoreComponent17", StringType(), True),
            StructField("ScoreComponent18", StringType(), True),
            StructField("ScoreComponent19", StringType(), True),
            StructField("ScoreComponent20", StringType(), True),
            StructField("AlertRelatedListOrderClordIds", StringType(), True),
            StructField("AlertRelatedListExecutionClordIds", StringType(), True)
        ])
        
        alerts_df_spark = spark.createDataFrame([], schema=alert_schema)
        print("RESULT: Created empty DataFrame with proper schema")
        
    else:
        # Parse the JSON string and convert to DataFrame
        print("INPUT: Converting JSON string to DataFrame")
        
        # Clean and parse JSON with better error handling for interactive runtime
        clean_json = input_dataframe_json_str.strip()
        
        # Try different JSON parsing approaches for interactive runtime
        try:
            # Method 1: Direct JSON parsing
            json_data = json.loads(clean_json)
            if isinstance(json_data, dict):
                json_data = [json_data]
            
            # Convert to pandas first, then to Spark (better for interactive runtime)
            pandas_df = pd.DataFrame(json_data)
            alerts_df_spark = spark.createDataFrame(pandas_df)
            
        except json.JSONDecodeError as je:
            print(f"JSON parsing error: {je}")
            # Method 2: Using Spark's JSON reader with RDD
            alerts_df_spark = spark.read.json(sc.parallelize([clean_json]))
            
        print(f"RESULT: Successfully created DataFrame with {alerts_df_spark.count()} rows")
        
        # Show the data structure (limit output for interactive runtime)
        print("DATAFRAME SCHEMA:")
        alerts_df_spark.printSchema()
        
        print("DATAFRAME CONTENT (first 3 rows):")
        alerts_df_spark.show(3, truncate=False)
        
        # Convert all columns to string type to match database requirements
        for column_name in alerts_df_spark.columns:
            alerts_df_spark = alerts_df_spark.withColumn(column_name, col(column_name).cast(StringType()))
        
        print("RESULT: All columns converted to StringType for database compatibility")

except Exception as e:
    print(f"ERROR: Failed to reconstruct DataFrame from JSON: {e}")
    print(f"ERROR TYPE: {type(e).__name__}")
    print("CREATING EMPTY DATAFRAME FOR GRACEFUL HANDLING")
    
    # Create empty dataframe instead of failing
    alert_schema = StructType([StructField("SMBCAIDPrefix", StringType(), True)])
    alerts_df_spark = spark.createDataFrame([], schema=alert_schema)

# Only proceed if DataFrame has data
if not alerts_df_spark.isEmpty():
    print("=== PROCESSING ALERTS FOR DATABASE SIMULATION ===")
    
    # Simulate database connection details (adapted for interactive runtime)
    sql_server_tdr = 'sql-tdr-dev-use2'
    database_name_tdr = 'sqldb-tdr-dev-use2'
    print(f"SIMULATION: Would connect to {sql_server_tdr}/{database_name_tdr}")
    
    # Simulate database operations with enhanced logging for interactive runtime
    print("=== SIMULATION: AlertDetail Table Insert ===")
    alert_count = alerts_df_spark.count()
    print(f"SIMULATION: Would insert {alert_count} rows into ccams.AlertDetail table")
    
    # Show sample of data that would be inserted
    print("SAMPLE: Data that would be inserted into AlertDetail:")
    alerts_df_spark.select("SMBCAIDPrefix", "ProductSymbol", "SMBCAlertType", "AccountID").show(3, truncate=False)
    
    # Create dummy AlertID (simulate what database would generate)
    base_alert_id = random.randint(10000, 99999)
    print(f"SIMULATION: Generated dummy AlertID: {base_alert_id}")
    
    # Simulate AlertLink table insert
    print("=== SIMULATION: AlertLink Table Insert ===")
    alertLink_data = {
        'AlertID': [base_alert_id],
        'SurveillanceRunId': [int(input_survid.split('_')[-1]) if input_survid.split('_')[-1].isdigit() else 12345],
        'DataRecordID': [int(input_ds_runid.split('_')[-1]) if input_ds_runid.split('_')[-1].isdigit() else 67890]
    }
    
    alertLink_df = pd.DataFrame(alertLink_data)
    print("SIMULATION: AlertLink data that would be inserted:")
    print(alertLink_df.to_string(index=False))
    
    # Simulate AlertTransactionLink table processing
    print("=== SIMULATION: AlertTransactionLink Table Processing ===")
    
    # Check if execution ClOrdIDs column exists
    execution_col = "AlertRelatedListExecutionClordIds"
    if execution_col in alerts_df_spark.columns:
        print(f"FOUND: {execution_col} column exists in DataFrame")
        
        # Filter for valid execution IDs
        valid_alerts_df = alerts_df_spark.filter(
            (col(execution_col).isNotNull()) & 
            (col(execution_col) != "") &
            (col(execution_col) != "N/A")
        )
        
        valid_count = valid_alerts_df.count()
        print(f"FOUND: {valid_count} alerts with valid execution ClOrdIDs")
        
        if valid_count > 0:
            # Show sample of execution IDs
            print("SAMPLE: Execution ClOrdIDs found:")
            valid_alerts_df.select(execution_col).show(2, truncate=False)
            
            # Simulate the explosion and processing of ClOrdIDs
            exploded_df = valid_alerts_df.withColumn(
                "ClOrdIDs_Array",
                split(regexp_replace(col(execution_col), "\\[|\\]", ""), ",")
            )
            
            transaction_links_df = exploded_df.select(
                lit(base_alert_id).alias("AlertID"),
                explode(col("ClOrdIDs_Array")).alias("ID")
            ).withColumn("Type", lit("Execution"))
            
            # Clean up whitespace
            transaction_links_df = transaction_links_df.withColumn(
                "ID", 
                regexp_replace(col("ID"), "\\s+", "")
            ).filter(col("ID") != "")
            
            transaction_link_count = transaction_links_df.count()
            print(f"SIMULATION: Would insert {transaction_link_count} rows into ccams.AlertTransactionLink")
            
            if transaction_link_count > 0:
                print("SAMPLE: Transaction links that would be created:")
                transaction_links_df.show(5, truncate=False)
            
        else:
            print("RESULT: No valid execution ClOrdIDs found for transaction links")
    else:
        print(f"WARNING: Column {execution_col} not found in DataFrame")
    
    # Generate comprehensive output summary
    print("=== FINAL SIMULATION SUMMARY ===")
    print(f"Runtime Mode: Interactive Runtime")
    print(f"Alert Type Processed: {input_alert_type}")
    print(f"Surveillance Run ID: {input_survid}")
    print(f"Dataset Run ID: {input_ds_runid}")
    print(f"Total Alerts Processed: {alert_count}")
    print(f"Simulated AlertID Generated: {base_alert_id}")
    print(f"AlertDetail Records: {alert_count} (simulated)")
    print(f"AlertLink Records: 1 (simulated)")
    
    if execution_col in alerts_df_spark.columns and valid_count > 0:
        print(f"AlertTransactionLink Records: {transaction_link_count} (simulated)")
    else:
        print("AlertTransactionLink Records: 0 (no execution IDs)")
    
    # Create detailed processing log
    processing_log = {
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "pipeline_type": "Common_Write_to_DB_Notebook",
        "runtime_mode": "Interactive Runtime",
        "alert_type": input_alert_type,
        "surveillance_run_id": input_survid,
        "dataset_run_id": input_ds_runid,
        "alerts_processed": alert_count,
        "simulated_alert_id": base_alert_id,
        "status": "SUCCESS_SIMULATION_INTERACTIVE"
    }
    
    print("=== PROCESSING LOG ===")
    for key, value in processing_log.items():
        print(f"{key}: {value}")

else:
    print("=== NO DATA TO PROCESS ===")
    print("INPUT: DataFrame is empty, skipping all database operations")
    
    # Still create a processing log for empty case
    processing_log = {
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "pipeline_type": "Common_Write_to_DB_Notebook",
        "runtime_mode": "Interactive Runtime",
        "alert_type": input_alert_type,
        "surveillance_run_id": input_survid,
        "dataset_run_id": input_ds_runid,
        "alerts_processed": 0,
        "simulated_alert_id": "N/A",
        "status": "SUCCESS_NO_DATA_INTERACTIVE"
    }
    
    print("=== PROCESSING LOG ===")
    for key, value in processing_log.items():
        print(f"{key}: {value}")

# Interactive Runtime Compatible Exit
print("=== NOTEBOOK EXECUTION COMPLETED ===")
print("STATUS: All database operations simulated successfully")
print("RUNTIME: Interactive Runtime compatible execution")
print("NEXT: Pipeline will continue to next activity")

# Try to exit with success message (compatible with both interactive and job modes)
try:
    dbutils.notebook.exit("SUCCESS: Database write simulation completed - Interactive Runtime")
except Exception as exit_error:
    print(f"Exit method not available in current context: {exit_error}")
    print("NOTEBOOK COMPLETED SUCCESSFULLY - Interactive Runtime Mode")
    
print("=== END OF NOTEBOOK ===")
