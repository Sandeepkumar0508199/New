# Databricks Notebook: Common_Write_to_DB_Notebook
# This notebook handles safe, sequential writing of alerts to the database.
# MODIFIED FOR DEV: Creates dummy data instead of actual database writes

import pandas as pd
from pyspark.sql.functions import col, lit, split, explode, regexp_replace
from pyspark.sql.types import StructType, StructField, StringType
import json
import datetime
import random

# Fetch Inputs from ADF - These are the parameters passed from calling pipeline
input_dataframe_json_str = dbutils.widgets.get("input_dataframe_json")
input_survid = dbutils.widgets.get("input_survid")
input_ds_runid = dbutils.widgets.get("input_ds_runid")
input_alert_type = dbutils.widgets.get("input_alert_type")

print("=== COMMON WRITE TO DB NOTEBOOK STARTED ===")
print(f"Received Surveillance ID: {input_survid}")
print(f"Received Dataset Run ID: {input_ds_runid}")
print(f"Received Alert Type: {input_alert_type}")
print(f"Received JSON Data Length: {len(input_dataframe_json_str)} characters")

# Reconstruct DataFrame from JSON string
try:
    # Check if input is empty or just empty brackets
    if not input_dataframe_json_str or input_dataframe_json_str.strip() == '[]':
        print("INPUT: Empty DataFrame received - no alerts to process")
        
        # Create empty schema matching AlertDetail table structure
        alert_schema = StructType([
            StructField("SMBCAIDPrefix", StringType(), True),
            StructField("ProductSymbol", StringType(), True),
            StructField("SMBCALertTime", StringType(), True),
            StructField("SMBCLocalAlertTime", StringType(), True),
            StructField("SMBCAlertType", StringType(), True),
            StructField("IssuerName", StringType(), True),
            StructField("ShortAlertDescription", StringType(), True),
            StructField("TotalScore", StringType(), True),
            StructField("BusinessUnit", StringType(), True),
            StructField("SMBCAlertGenerationDate", StringType(), True),
            StructField("ProductName", StringType(), True),
            StructField("ProductID", StringType(), True),
            StructField("AccountID", StringType(), True),
            StructField("Exchange", StringType(), True),
            StructField("AssetClass", StringType(), True),
            StructField("LongDescription", StringType(), True),
            StructField("AnalyticsValue1", StringType(), True),
            StructField("AnalyticsValue2", StringType(), True),
            StructField("AnalyticsValue3", StringType(), True),
            StructField("AnalyticsValue4", StringType(), True),
            StructField("AnalyticsValue5", StringType(), True),
            StructField("AnalyticsValue6", StringType(), True),
            StructField("AnalyticsValue7", StringType(), True),
            StructField("AnalyticsValue8", StringType(), True),
            StructField("AnalyticsValue9", StringType(), True),
            StructField("AnalyticsValue10", StringType(), True),
            StructField("AnalyticsValue11", StringType(), True),
            StructField("AnalyticsValue12", StringType(), True),
            StructField("AnalyticsValue13", StringType(), True),
            StructField("AnalyticsValue14", StringType(), True),
            StructField("AnalyticsValue15", StringType(), True),
            StructField("AnalyticsValue16", StringType(), True),
            StructField("AnalyticsValue17", StringType(), True),
            StructField("AnalyticsValue18", StringType(), True),
            StructField("AnalyticsValue19", StringType(), True),
            StructField("AnalyticsValue20", StringType(), True),
            StructField("ThresholdValue1", StringType(), True),
            StructField("ThresholdValue2", StringType(), True),
            StructField("ThresholdValue3", StringType(), True),
            StructField("ThresholdValue4", StringType(), True),
            StructField("ThresholdValue5", StringType(), True),
            StructField("ThresholdValue6", StringType(), True),
            StructField("ThresholdValue7", StringType(), True),
            StructField("ThresholdValue8", StringType(), True),
            StructField("ThresholdValue9", StringType(), True),
            StructField("ThresholdValue10", StringType(), True),
            StructField("ThresholdValue11", StringType(), True),
            StructField("ThresholdValue12", StringType(), True),
            StructField("ThresholdValue13", StringType(), True),
            StructField("ThresholdValue14", StringType(), True),
            StructField("ThresholdValue15", StringType(), True),
            StructField("ThresholdValue16", StringType(), True),
            StructField("ThresholdValue17", StringType(), True),
            StructField("ThresholdValue18", StringType(), True),
            StructField("ThresholdValue19", StringType(), True),
            StructField("ThresholdValue20", StringType(), True),
            StructField("ScoreComponent1", StringType(), True),
            StructField("ScoreComponent2", StringType(), True),
            StructField("ScoreComponent3", StringType(), True),
            StructField("ScoreComponent4", StringType(), True),
            StructField("ScoreComponent5", StringType(), True),
            StructField("ScoreComponent6", StringType(), True),
            StructField("ScoreComponent7", StringType(), True),
            StructField("ScoreComponent8", StringType(), True),
            StructField("ScoreComponent9", StringType(), True),
            StructField("ScoreComponent10", StringType(), True),
            StructField("ScoreComponent11", StringType(), True),
            StructField("ScoreComponent12", StringType(), True),
            StructField("ScoreComponent13", StringType(), True),
            StructField("ScoreComponent14", StringType(), True),
            StructField("ScoreComponent15", StringType(), True),
            StructField("ScoreComponent16", StringType(), True),
            StructField("ScoreComponent17", StringType(), True),
            StructField("ScoreComponent18", StringType(), True),
            StructField("ScoreComponent19", StringType(), True),
            StructField("ScoreComponent20", StringType(), True),
            StructField("AlertRelatedListOrderClordIds", StringType(), True),
            StructField("AlertRelatedListExecutionClordIds", StringType(), True)
        ])
        
        alerts_df_spark = spark.createDataFrame([], schema=alert_schema)
        print("RESULT: Created empty DataFrame with proper schema")
        
    else:
        # Parse the JSON string and convert to DataFrame
        print("INPUT: Converting JSON string to DataFrame")
        alerts_df_spark = spark.read.json(sc.parallelize([input_dataframe_json_str]))
        print(f"RESULT: Successfully created DataFrame with {alerts_df_spark.count()} rows")
        
        # Show the data structure
        print("DATAFRAME SCHEMA:")
        alerts_df_spark.printSchema()
        
        print("DATAFRAME CONTENT (first 5 rows):")
        alerts_df_spark.show(5, truncate=False)
        
        # Convert all columns to string type to match database requirements
        for column_name in alerts_df_spark.columns:
            alerts_df_spark = alerts_df_spark.withColumn(column_name, col(column_name).cast(StringType()))
        
        print("RESULT: All columns converted to StringType")

except Exception as e:
    print(f"ERROR: Failed to reconstruct DataFrame from JSON: {e}")
    dbutils.notebook.exit(f"FAILED: Cannot process input data: {e}")

# Only proceed if DataFrame has data
if not alerts_df_spark.isEmpty():
    print("=== PROCESSING ALERTS FOR DATABASE SIMULATION ===")
    
    # Simulate database connection details (not actually connecting)
    sql_server_tdr = 'sql-tdr-dev-use2'
    database_name_tdr = 'sqldb-tdr-dev-use2'
    print(f"SIMULATION: Would connect to {sql_server_tdr}/{database_name_tdr}")
    
    # Simulate database operations instead of actual writes
    print("=== SIMULATION: AlertDetail Table Insert ===")
    alert_count = alerts_df_spark.count()
    print(f"SIMULATION: Would insert {alert_count} rows into ccams.AlertDetail table")
    
    # Create dummy AlertID (simulate what database would generate)
    base_alert_id = random.randint(10000, 99999)
    print(f"SIMULATION: Generated dummy AlertID: {base_alert_id}")
    
    # Simulate AlertLink table insert
    print("=== SIMULATION: AlertLink Table Insert ===")
    alertLink_data = {
        'AlertID': [base_alert_id],
        'SurveillanceRunId': [int(input_survid) if input_survid.isdigit() else 12345],
        'DataRecordID': [int(input_ds_runid) if input_ds_runid.isdigit() else 67890]
    }
    
    alertLink_df = pd.DataFrame(alertLink_data)
    print("SIMULATION: AlertLink data that would be inserted:")
    print(alertLink_df)
    
    # Simulate AlertTransactionLink table processing
    print("=== SIMULATION: AlertTransactionLink Table Processing ===")
    
    # Check if execution ClOrdIDs column exists
    execution_col = "AlertRelatedListExecutionClordIds"
    if execution_col in alerts_df_spark.columns:
        print(f"FOUND: {execution_col} column exists in DataFrame")
        
        # Filter for valid execution IDs
        valid_alerts_df = alerts_df_spark.filter(
            (col(execution_col).isNotNull()) & 
            (col(execution_col) != "") &
            (col(execution_col) != "N/A")
        )
        
        valid_count = valid_alerts_df.count()
        print(f"FOUND: {valid_count} alerts with valid execution ClOrdIDs")
        
        if valid_count > 0:
            # Show sample of execution IDs
            print("SAMPLE: Execution ClOrdIDs found:")
            valid_alerts_df.select(execution_col).show(3, truncate=False)
            
            # Simulate the explosion and processing of ClOrdIDs
            exploded_df = valid_alerts_df.withColumn(
                "ClOrdIDs_Array",
                split(regexp_replace(col(execution_col), "\\[|\\]", ""), ",")
            )
            
            transaction_links_df = exploded_df.select(
                lit(base_alert_id).alias("AlertID"),
                explode(col("ClOrdIDs_Array")).alias("ID")
            ).withColumn("Type", lit("Execution"))
            
            # Clean up whitespace
            transaction_links_df = transaction_links_df.withColumn(
                "ID", 
                regexp_replace(col("ID"), "\\s+", "")
            ).filter(col("ID") != "")
            
            transaction_link_count = transaction_links_df.count()
            print(f"SIMULATION: Would insert {transaction_link_count} rows into ccams.AlertTransactionLink")
            
            if transaction_link_count > 0:
                print("SAMPLE: Transaction links that would be created:")
                transaction_links_df.show(5, truncate=False)
            
        else:
            print("RESULT: No valid execution ClOrdIDs found for transaction links")
    else:
        print(f"WARNING: Column {execution_col} not found in DataFrame")
    
    # Generate comprehensive output summary
    print("=== FINAL SIMULATION SUMMARY ===")
    print(f"Alert Type Processed: {input_alert_type}")
    print(f"Surveillance Run ID: {input_survid}")
    print(f"Dataset Run ID: {input_ds_runid}")
    print(f"Total Alerts Processed: {alert_count}")
    print(f"Simulated AlertID Generated: {base_alert_id}")
    print(f"AlertDetail Records: {alert_count} (simulated)")
    print(f"AlertLink Records: 1 (simulated)")
    
    if execution_col in alerts_df_spark.columns:
        transaction_count = valid_alerts_df.count()
        print(f"AlertTransactionLink Records: {transaction_count} (simulated)")
    else:
        print("AlertTransactionLink Records: 0 (no execution IDs)")
    
    # Create detailed processing log
    processing_log = {
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "pipeline_type": "Common_Write_to_DB_Notebook",
        "alert_type": input_alert_type,
        "surveillance_run_id": input_survid,
        "dataset_run_id": input_ds_runid,
        "alerts_processed": alert_count,
        "simulated_alert_id": base_alert_id,
        "status": "SUCCESS_SIMULATION"
    }
    
    print("=== PROCESSING LOG ===")
    for key, value in processing_log.items():
        print(f"{key}: {value}")

else:
    print("=== NO DATA TO PROCESS ===")
    print("INPUT: DataFrame is empty, skipping all database operations")
    
    # Still create a processing log for empty case
    processing_log = {
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "pipeline_type": "Common_Write_to_DB_Notebook",
        "alert_type": input_alert_type,
        "surveillance_run_id": input_survid,
        "dataset_run_id": input_ds_runid,
        "alerts_processed": 0,
        "simulated_alert_id": "N/A",
        "status": "SUCCESS_NO_DATA"
    }
    
    print("=== PROCESSING LOG ===")
    for key, value in processing_log.items():
        print(f"{key}: {value}")

# Simulate successful completion
print("=== NOTEBOOK EXECUTION COMPLETED ===")
print("STATUS: All database operations simulated successfully")
print("NEXT: Pipeline will continue to next activity")

# Exit with success message (this tells ADF the notebook completed successfully)
dbutils.notebook.exit("SUCCESS: Database write simulation completed")
