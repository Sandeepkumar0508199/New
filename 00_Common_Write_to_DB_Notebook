# Databricks Notebook: Common_Write_to_DB_Notebook
import pandas as pd
import json
import urllib.parse
from pyspark.sql.functions import col, lit, split, explode, regexp_replace
from pyspark.sql.types import StructType, StructField, StringType

# Define alert schema
ALERT_SCHEMA = StructType([
    StructField("SMBCAIDPrefix", StringType(), True),
    StructField("ProductSymbol", StringType(), True),
    StructField("SMBCALertTime", StringType(), True),
    StructField("SMBCLocalAlertTime", StringType(), True),
    StructField("SMBCAlertType", StringType(), True),
    StructField("IssuerName", StringType(), True),
    StructField("ShortAlertDescription", StringType(), True),
    StructField("TotalScore", StringType(), True),
    StructField("BusinessUnit", StringType(), True),
    StructField("SMBCAlertGenerationDate", StringType(), True),
    StructField("ProductName", StringType(), True),
    StructField("ProductID", StringType(), True),
    StructField("AccountID", StringType(), True),
    StructField("Exchange", StringType(), True),
    StructField("AssetClass", StringType(), True),
    StructField("LongDescription", StringType(), True),
    StructField("AnalyticsValue1", StringType(), True),
    StructField("AnalyticsValue2", StringType(), True),
    StructField("AnalyticsValue3", StringType(), True),
    StructField("AnalyticsValue4", StringType(), True),
    StructField("AnalyticsValue5", StringType(), True),
    StructField("AnalyticsValue6", StringType(), True),
    StructField("AnalyticsValue7", StringType(), True),
    StructField("AnalyticsValue8", StringType(), True),
    StructField("AnalyticsValue9", StringType(), True),
    StructField("AnalyticsValue10", StringType(), True),
    StructField("AnalyticsValue11", StringType(), True),
    StructField("AnalyticsValue12", StringType(), True),
    StructField("AnalyticsValue13", StringType(), True),
    StructField("AnalyticsValue14", StringType(), True),
    StructField("AnalyticsValue15", StringType(), True),
    StructField("AnalyticsValue16", StringType(), True),
    StructField("AnalyticsValue17", StringType(), True),
    StructField("AnalyticsValue18", StringType(), True),
    StructField("AnalyticsValue19", StringType(), True),
    StructField("AnalyticsValue20", StringType(), True),
    StructField("ThresholdValue1", StringType(), True),
    StructField("ThresholdValue2", StringType(), True),
    StructField("ThresholdValue3", StringType(), True),
    StructField("ThresholdValue4", StringType(), True),
    StructField("ThresholdValue5", StringType(), True),
    StructField("ThresholdValue6", StringType(), True),
    StructField("ThresholdValue7", StringType(), True),
    StructField("ThresholdValue8", StringType(), True),
    StructField("ThresholdValue9", StringType(), True),
    StructField("ThresholdValue10", StringType(), True),
    StructField("ThresholdValue11", StringType(), True),
    StructField("ThresholdValue12", StringType(), True),
    StructField("ThresholdValue13", StringType(), True),
    StructField("ThresholdValue14", StringType(), True),
    StructField("ThresholdValue15", StringType(), True),
    StructField("ThresholdValue16", StringType(), True),
    StructField("ThresholdValue17", StringType(), True),
    StructField("ThresholdValue18", StringType(), True),
    StructField("ThresholdValue19", StringType(), True),
    StructField("ThresholdValue20", StringType(), True),
    StructField("ScoreComponent1", StringType(), True),
    StructField("ScoreComponent2", StringType(), True),
    StructField("ScoreComponent3", StringType(), True),
    StructField("ScoreComponent4", StringType(), True),
    StructField("ScoreComponent5", StringType(), True),
    StructField("ScoreComponent6", StringType(), True),
    StructField("ScoreComponent7", StringType(), True),
    StructField("ScoreComponent8", StringType(), True),
    StructField("ScoreComponent9", StringType(), True),
    StructField("ScoreComponent10", StringType(), True),
    StructField("ScoreComponent11", StringType(), True),
    StructField("ScoreComponent12", StringType(), True),
    StructField("ScoreComponent13", StringType(), True),
    StructField("ScoreComponent14", StringType(), True),
    StructField("ScoreComponent15", StringType(), True),
    StructField("ScoreComponent16", StringType(), True),
    StructField("ScoreComponent17", StringType(), True),
    StructField("ScoreComponent18", StringType(), True),
    StructField("ScoreComponent19", StringType(), True),
    StructField("ScoreComponent20", StringType(), True),
    StructField("AlertRelatedListOrderClordIds", StringType(), True),
    StructField("AlertRelatedListExecutionClordIds", StringType(), True)
])

# Get inputs from ADF
input_json = dbutils.widgets.get("input_dataframe_json")
survid = dbutils.widgets.get("input_survid") 
ds_runid = dbutils.widgets.get("input_ds_runid")
alert_type = dbutils.widgets.get("input_alert_type")

print(f"Received job - Surveillance ID: {survid}, Alert Type: {alert_type}")

# Process JSON input
if not input_json or input_json.strip() in ['[]', '{}', 'null']:
    alerts_df = spark.createDataFrame([], schema=ALERT_SCHEMA)
    print("No alerts to process")
else:
    try:
        clean_input = input_json.strip()
        decoded_json = urllib.parse.unquote(clean_input)
        json_data = json.loads(decoded_json)
        
        if isinstance(json_data, dict):
            json_data = [json_data]
        elif not isinstance(json_data, list):
            json_data = []
        
        if len(json_data) > 0:
            pandas_df = pd.DataFrame(json_data)
            temp_df = spark.createDataFrame(pandas_df)
            
            columns = []
            for field in ALERT_SCHEMA.fields:
                if field.name in temp_df.columns:
                    columns.append(col(field.name).cast(StringType()).alias(field.name))
                else:
                    columns.append(lit(None).cast(StringType()).alias(field.name))
            
            alerts_df = temp_df.select(*columns)
            print(f"Processed {alerts_df.count()} alerts")
        else:
            alerts_df = spark.createDataFrame([], schema=ALERT_SCHEMA)
            print("Empty JSON data")
            
    except Exception as e:
        print(f"Error processing JSON: {e}")
        alerts_df = spark.createDataFrame([], schema=ALERT_SCHEMA)

# Database operations
if alerts_df.count() > 0:
    print(f"Writing {alerts_df.count()} alerts to database...")
    
    # Database connection
    sql_server_tdr = 'sql-tdr-qa-use2'
    database_name_tdr = 'sqldb-tdr-qa-use2'
    test_sp_name_tdr = dbutils.secrets.get("ccams-scope", "ccams-app-reg-id")
    test_sp_pwd_tdr = dbutils.secrets.get("ccams-scope", "ccams-app-reg-sec")
    
    sql_server_fqdn_tdr = f"jdbc:sqlserver://{sql_server_tdr}.database.windows.net:1433"
    jdbc_parms_tdr = (
        "encrypt=true;",
        "trustServerCertificate=true;", 
        "hostNameInCertificate=*.database.windows.net;",
        "loginTimeout=30;",
        "driver=com.microsoft.sqlserver.jdbc.SQLServerDriver;",
        "authentication=ActiveDirectoryServicePrincipal"
    )
    connection_string_tdr = f"jdbc:sqlserver://{sql_server_fqdn_tdr};database={database_name_tdr};user={test_sp_name_tdr};password={test_sp_pwd_tdr};{'.'.join(jdbc_parms_tdr)}"
    
    # Write to AlertDetail
    print("Writing to ccams.AlertDetail...")
    alerts_df.write.format("jdbc").mode("append").option("url", connection_string_tdr).option("dbtable", "ccams.AlertDetail").save()
    print("Successfully wrote to AlertDetail")
    
    # Get AlertID
    print("Getting new AlertID...")
    AlertID_query = f"SELECT MAX(AlertID) as AlertID FROM ccams.AlertDetail WHERE SMBCAlertType = '{alert_type}'"
    AlertID_df = (spark.read
        .format("jdbc")
        .option("url", connection_string_tdr)
        .option("query", AlertID_query)
        .load()
    )
    AlertID = AlertID_df.toPandas()['AlertID'].iloc[0]
    print(f"Retrieved AlertID: {AlertID}")
    
    # Write to AlertLink
    print("Writing to ccams.AlertLink...")
    link_data = pd.DataFrame({
        'AlertID': [AlertID],
        'SurveillanceRunId': [survid],
        'DataRecordID': [ds_runid]
    })
    link_df = spark.createDataFrame(link_data)
    link_df.write.format("jdbc").mode("append").option("url", connection_string_tdr).option("dbtable", "ccams.AlertLink").save()
    print("Successfully wrote to AlertLink")
    
    # Write to AlertTransactionLink
    print("Writing to ccams.AlertTransactionLink...")
    execution_col = "AlertRelatedListExecutionClordIds"
    if execution_col in alerts_df.columns:
        valid_executions = alerts_df.filter(
            (col(execution_col).isNotNull()) & 
            (col(execution_col) != "") & 
            (col(execution_col) != "N/A")
        )
        
        if valid_executions.count() > 0:
            exploded = valid_executions.withColumn(
                "exec_array",
                split(regexp_replace(col(execution_col), "\\[|\\]", ""), ",")
            )
            
            transaction_links = exploded.select(
                lit(AlertID).alias("AlertID"),
                explode(col("exec_array")).alias("ID")
            ).withColumn("Type", lit("Execution"))
            
            transaction_links = transaction_links.withColumn(
                "ID",
                regexp_replace(col("ID"), "\\s+", "")
            ).filter(
                (col("ID") != "") & 
                (col("ID") != "N/A") & 
                (col("ID").isNotNull())
            )
            
            if transaction_links.count() > 0:
                transaction_links.write.format("jdbc").mode("append").option("url", connection_string_tdr).option("dbtable", "ccams.AlertTransactionLink").save()
                print(f"Successfully wrote {transaction_links.count()} transaction links")
            else:
                print("No valid execution IDs to link")
        else:
            print("No alerts with execution IDs found")
    else:
        print("No execution ID column found")

else:
    print("No alerts to write - skipping database operations")

print("Database writing completed successfully!")
dbutils.notebook.exit("SUCCESS")
