import pandas as pd
import json
import datetime as dt
import urllib.parse
from pyspark.sql.functions import col, lit, split, explode, regexp_replace
from pyspark.sql.types import StructType, StructField, StringType

UNIVERSAL_ALERT_SCHEMA = StructType([
    StructField("SMBCAIDPrefix", StringType(), True),
    StructField("ProductSymbol", StringType(), True),
    StructField("SMBCALertTime", StringType(), True),
    StructField("SMBCLocalAlertTime", StringType(), True),
    StructField("SMBCAlertType", StringType(), True),
    StructField("IssuerName", StringType(), True),
    StructField("ShortAlertDescription", StringType(), True),
    StructField("TotalScore", StringType(), True),
    StructField("BusinessUnit", StringType(), True),
    StructField("SMBCAlertGenerationDate", StringType(), True),
    StructField("ProductName", StringType(), True),
    StructField("ProductID", StringType(), True),
    StructField("AccountID", StringType(), True),
    StructField("Exchange", StringType(), True),
    StructField("AssetClass", StringType(), True),
    StructField("LongDescription", StringType(), True),
    StructField("AnalyticsValue1", StringType(), True),
    StructField("AnalyticsValue2", StringType(), True),
    StructField("AnalyticsValue3", StringType(), True),
    StructField("AnalyticsValue4", StringType(), True),
    StructField("AnalyticsValue5", StringType(), True),
    StructField("AnalyticsValue6", StringType(), True),
    StructField("AnalyticsValue7", StringType(), True),
    StructField("AnalyticsValue8", StringType(), True),
    StructField("AnalyticsValue9", StringType(), True),
    StructField("AnalyticsValue10", StringType(), True),
    StructField("AnalyticsValue11", StringType(), True),
    StructField("AnalyticsValue12", StringType(), True),
    StructField("AnalyticsValue13", StringType(), True),
    StructField("AnalyticsValue14", StringType(), True),
    StructField("AnalyticsValue15", StringType(), True),
    StructField("AnalyticsValue16", StringType(), True),
    StructField("AnalyticsValue17", StringType(), True),
    StructField("AnalyticsValue18", StringType(), True),
    StructField("AnalyticsValue19", StringType(), True),
    StructField("AnalyticsValue20", StringType(), True),
    StructField("ThresholdValue1", StringType(), True),
    StructField("ThresholdValue2", StringType(), True),
    StructField("ThresholdValue3", StringType(), True),
    StructField("ThresholdValue4", StringType(), True),
    StructField("ThresholdValue5", StringType(), True),
    StructField("ThresholdValue6", StringType(), True),
    StructField("ThresholdValue7", StringType(), True),
    StructField("ThresholdValue8", StringType(), True),
    StructField("ThresholdValue9", StringType(), True),
    StructField("ThresholdValue10", StringType(), True),
    StructField("ThresholdValue11", StringType(), True),
    StructField("ThresholdValue12", StringType(), True),
    StructField("ThresholdValue13", StringType(), True),
    StructField("ThresholdValue14", StringType(), True),
    StructField("ThresholdValue15", StringType(), True),
    StructField("ThresholdValue16", StringType(), True),
    StructField("ThresholdValue17", StringType(), True),
    StructField("ThresholdValue18", StringType(), True),
    StructField("ThresholdValue19", StringType(), True),
    StructField("ThresholdValue20", StringType(), True),
    StructField("ScoreComponent1", StringType(), True),
    StructField("ScoreComponent2", StringType(), True),
    StructField("ScoreComponent3", StringType(), True),
    StructField("ScoreComponent4", StringType(), True),
    StructField("ScoreComponent5", StringType(), True),
    StructField("ScoreComponent6", StringType(), True),
    StructField("ScoreComponent7", StringType(), True),
    StructField("ScoreComponent8", StringType(), True),
    StructField("ScoreComponent9", StringType(), True),
    StructField("ScoreComponent10", StringType(), True),
    StructField("ScoreComponent11", StringType(), True),
    StructField("ScoreComponent12", StringType(), True),
    StructField("ScoreComponent13", StringType(), True),
    StructField("ScoreComponent14", StringType(), True),
    StructField("ScoreComponent15", StringType(), True),
    StructField("ScoreComponent16", StringType(), True),
    StructField("ScoreComponent17", StringType(), True),
    StructField("ScoreComponent18", StringType(), True),
    StructField("ScoreComponent19", StringType(), True),
    StructField("ScoreComponent20", StringType(), True),
    StructField("AlertRelatedListOrderClordIds", StringType(), True),
    StructField("AlertRelatedListExecutionClordIds", StringType(), True)
])

try:
    input_dataframe_json_str = dbutils.widgets.get("input_dataframe_json")
    input_survid = dbutils.widgets.get("input_survid")
    input_ds_runid = dbutils.widgets.get("input_ds_runid")
    input_alert_type = dbutils.widgets.get("input_alert_type")
except Exception as e:
    dbutils.notebook.exit(f"STOP: Cannot get pipeline messages: {e}")

try:
    if not input_dataframe_json_str:
        alerts_df_spark = spark.createDataFrame([], schema=UNIVERSAL_ALERT_SCHEMA)
    else:
        cleaned_input = input_dataframe_json_str.strip()
        
        if cleaned_input in ['', '[]', '{}', 'null', '""', "''", 'None']:
            alerts_df_spark = spark.createDataFrame([], schema=UNIVERSAL_ALERT_SCHEMA)
        else:
            decoded_json_str = urllib.parse.unquote(cleaned_input)
            decoded_json_str = decoded_json_str.strip()
            
            if not decoded_json_str or decoded_json_str in ['[]', '{}', 'null', '""', "''", 'None']:
                alerts_df_spark = spark.createDataFrame([], schema=UNIVERSAL_ALERT_SCHEMA)
            else:
                try:
                    json_data = json.loads(decoded_json_str)
                    
                    if json_data is None:
                        json_data = []
                    elif isinstance(json_data, dict):
                        json_data = [json_data]
                    elif not isinstance(json_data, list):
                        json_data = []
                    
                    if len(json_data) > 0:
                        temp_pandas_df = pd.DataFrame(json_data)
                        temp_alerts_df_spark = spark.createDataFrame(temp_pandas_df)
                        
                        final_columns_selection = []
                        current_columns = temp_alerts_df_spark.columns
                        
                        for field in UNIVERSAL_ALERT_SCHEMA.fields:
                            if field.name in current_columns:
                                final_columns_selection.append(col(field.name).cast(StringType()).alias(field.name))
                            else:
                                final_columns_selection.append(lit(None).cast(StringType()).alias(field.name))

                        alerts_df_spark = temp_alerts_df_spark.select(*final_columns_selection)
                    else:
                        alerts_df_spark = spark.createDataFrame([], schema=UNIVERSAL_ALERT_SCHEMA)
                        
                except json.JSONDecodeError as e:
                    if decoded_json_str.startswith('"') and decoded_json_str.endswith('"') and len(decoded_json_str) > 2:
                        try:
                            inner_json = decoded_json_str[1:-1]
                            json_data = json.loads(inner_json)
                            
                            if isinstance(json_data, dict):
                                json_data = [json_data]
                            elif not isinstance(json_data, list):
                                json_data = []
                                
                            if len(json_data) > 0:
                                temp_pandas_df = pd.DataFrame(json_data)
                                temp_alerts_df_spark = spark.createDataFrame(temp_pandas_df)
                                
                                final_columns_selection = []
                                current_columns = temp_alerts_df_spark.columns
                                
                                for field in UNIVERSAL_ALERT_SCHEMA.fields:
                                    if field.name in current_columns:
                                        final_columns_selection.append(col(field.name).cast(StringType()).alias(field.name))
                                    else:
                                        final_columns_selection.append(lit(None).cast(StringType()).alias(field.name))

                                alerts_df_spark = temp_alerts_df_spark.select(*final_columns_selection)
                            else:
                                alerts_df_spark = spark.createDataFrame([], schema=UNIVERSAL_ALERT_SCHEMA)
                        except:
                            raise Exception(f"Invalid JSON format: {e}")
                    else:
                        raise Exception(f"Invalid JSON format: {e}")

except Exception as e:
    dbutils.notebook.exit(f"STOP: Cannot prepare alert data: {e}")

if not alerts_df_spark.isEmpty():
    sql_server_tdr = 'sql-tdr-qa-use2'
    database_name_tdr = 'sqldb-tdr-qa-use2'
    
    try:
        test_sp_name_tdr = dbutils.secrets.get("ccams-scope", "ccams-app-reg-id")
        test_sp_pwd_tdr = dbutils.secrets.get("ccams-scope", "ccams-app-reg-sec")
    except Exception as e:
        dbutils.notebook.exit(f"STOP: Cannot get database keys: {e}")

    sql_server_fqdn_tdr = f"jdbc:sqlserver://{sql_server_tdr}.database.windows.net:1433"
    jdbc_parms_tdr = (
        "encrypt=true;",
        "trustServerCertificate=true;",
        "hostNameInCertificate=*.database.windows.net;",
        "loginTimeout=30;",
        "driver=com.microsoft.sqlserver.jdbc.SQLServerDriver;",
        "authentication=ActiveDirectoryServicePrincipal"
    )
    connection_string_tdr = f"jdbc:sqlserver://{sql_server_fqdn_tdr};database={database_name_tdr};user={test_sp_name_tdr};password={test_sp_pwd_tdr};{''.join(jdbc_parms_tdr)}"

    try:
        print(f"--- Final alerts for 'sqldb-tdr-qa-use2.ccams.AlertDetail' ({alerts_df_spark.count()} reports) ---")
        alerts_df_spark.printSchema()
        alerts_df_spark.show(truncate=False)
    except Exception as e:
        dbutils.notebook.exit(f"STOP: Cannot process alerts for 'sqldb-tdr-qa-use2.ccams.AlertDetail': {e}")

    try:
        AlertID_query = f"SELECT MAX(AlertID) as AlertID FROM ccams.AlertDetail WHERE SMBCAlertType = '{input_alert_type}'"
        AlertID_df = (spark.read
        .format("jdbc")
        .option("url", connection_string_tdr)
        .option("query", AlertID_query)
        .load()
        )
        
        if AlertID_df.count() == 0 or AlertID_df.toPandas()['AlertID'].iloc[0] is None:
             AlertID = 1
        else:
            AlertID = AlertID_df.toPandas()['AlertID'].iloc[0]
    except Exception as e:
        AlertID = 1

    try:
        alertLink_data = {
            'AlertID': [AlertID],
            'SurveillanceRunId': [input_survid],
            'DataRecordID': [input_ds_runid]
        }
        alertLink_df_spark = spark.createDataFrame(pd.DataFrame(alertLink_data))
        
        print(f"--- Final alerts for 'sqldb-tdr-qa-use2.ccams.AlertLink' ({alertLink_df_spark.count()} reports) ---")
        alertLink_df_spark.printSchema()
        alertLink_df_spark.show(truncate=False)
    except Exception as e:
        dbutils.notebook.exit(f"STOP: Cannot process 'sqldb-tdr-qa-use2.ccams.AlertLink': {e}")

    try:
        if "AlertRelatedListExecutionClordIds" in alerts_df_spark.columns:
            valid_alerts_df = alerts_df_spark.filter(
                (col("AlertRelatedListExecutionClordIds").isNotNull()) &
                (col("AlertRelatedListExecutionClordIds") != "") &
                (col("AlertRelatedListExecutionClordIds") != "N/A")
            )

            if not valid_alerts_df.isEmpty():
                exploded_df = valid_alerts_df.withColumn(
                    "ClOrdIDs_Array",
                    split(regexp_replace(col("AlertRelatedListExecutionClordIds"), "\\[|\\]", ""), ",")
                )

                alert_transaction_link_spark = exploded_df.select(
                    lit(AlertID).alias("AlertID"),
                    explode(col("ClOrdIDs_Array")).alias("ID")
                ).withColumn("Type", lit("Execution"))

                alert_transaction_link_spark = alert_transaction_link_spark.withColumn(
                    "ID",
                    regexp_replace(col("ID"), "\\s+", "")
                )

                alert_transaction_link_spark = alert_transaction_link_spark.filter(
                    (col("ID") != "") & 
                    (col("ID") != "N/A") & 
                    (col("ID").isNotNull())
                )

                if not alert_transaction_link_spark.isEmpty():
                    print(f"--- Final alerts for 'sqldb-tdr-qa-use2.ccams.AlertTransactionLink' ({alert_transaction_link_spark.count()} reports) ---")
                    alert_transaction_link_spark.printSchema()
                    alert_transaction_link_spark.show(truncate=False)
    except Exception as e:
        print(f"Warning: Cannot process transaction links: {e}")

dbutils.notebook.exit("Success")
